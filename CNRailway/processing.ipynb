{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pinyin in c:\\users\\mjh01\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pinyin\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "dateTable = {\"第一天\": 0, \"第二天\": 1, \"第三天\": 2, \"第四天\": 3, \"第五天\": 4, \"第六天\": 5}\n",
    "\n",
    "with open('data/火车班次json数据', 'r', encoding = 'utf-8') as file:\n",
    "    for line in file:\n",
    "        currLine = json.loads(line)\n",
    "        # print(currLine)\n",
    "        currLineID = list(currLine[\"ticketInfo\"].keys())[0]\n",
    "        currLineStations = currLine[\"trainScheduleBody\"]\n",
    "        currLineMidSta = []\n",
    "        for sta in currLineStations:\n",
    "            # Time\n",
    "            if sta[\"content\"][3] == \"起点站\":\n",
    "                staTime = sta[\"content\"][4].split(\":\")\n",
    "                staCost = 0\n",
    "                staDist = 0\n",
    "                currLineMidSta.append((int(staTime[0]), int(staTime[1]), staCost, staDist, sta[\"content\"][1], currLineID))\n",
    "            else:\n",
    "                staTime = sta[\"content\"][3].split(\":\")\n",
    "                staCost = sta[\"content\"][7]\n",
    "                staDist = sta[\"content\"][6].split(\"公里\")\n",
    "                currLineMidSta.append((int(staTime[0]) + 24 * dateTable[sta[\"content\"][2]], int(staTime[1]), staCost, float(staDist[0]), sta[\"content\"][1], currLineID))\n",
    "\n",
    "        currLineMidSta = sorted(currLineMidSta)\n",
    "\n",
    "        for ind in range(len(currLineMidSta) - 1):\n",
    "            if currLineMidSta[ind][4] not in data:\n",
    "                data[currLineMidSta[ind][4]] = {}\n",
    "            if currLineMidSta[ind + 1][4] not in data:\n",
    "                data[currLineMidSta[ind + 1][4]] = {}\n",
    "            if currLineMidSta[ind + 1][1] < currLineMidSta[ind][1]:\n",
    "                currMinute = 60 - currLineMidSta[ind][1] + currLineMidSta[ind + 1][1]\n",
    "                currHour = currLineMidSta[ind + 1][0] - currLineMidSta[ind][0] - 1\n",
    "            else:\n",
    "                currMinute = currLineMidSta[ind + 1][1] - currLineMidSta[ind][1]\n",
    "                currHour = currLineMidSta[ind + 1][0] - currLineMidSta[ind][0]\n",
    "            if currLineMidSta[ind + 1][4] in data[currLineMidSta[ind][4]]:\n",
    "                if currHour > data[currLineMidSta[ind][4]][currLineMidSta[ind + 1][4]][0]:\n",
    "                    continue\n",
    "                elif currHour == data[currLineMidSta[ind][4]][currLineMidSta[ind + 1][4]][0]:\n",
    "                    if currMinute >= data[currLineMidSta[ind][4]][currLineMidSta[ind + 1][4]][1]:\n",
    "                        continue\n",
    "            \n",
    "            currCost = currLineMidSta[ind + 1][2] - currLineMidSta[ind][2]\n",
    "\n",
    "            currDist = currLineMidSta[ind + 1][3] - currLineMidSta[ind][3]\n",
    "\n",
    "            currLineID = currLineMidSta[ind][5]\n",
    "\n",
    "            data[currLineMidSta[ind][4]][currLineMidSta[ind + 1][4]] = (currHour, currMinute, abs(currCost), currDist, currLineID)\n",
    "            # data[currLineMidSta[ind + 1][4]][currLineMidSta[ind][4]] = (currHour, currMinute, abs(currCost), currDist, currLineID)\n",
    "        # break\n",
    "\n",
    "for dataKey in data.keys():\n",
    "    data[dataKey] = {k : v for k, v in sorted(data[dataKey].items(), key = lambda x : x[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data_final.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sta_cities.json', 'r', encoding='utf-8') as f:\n",
    "    cities = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikidata(station_name: str):\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',\n",
    "        'format': 'json',\n",
    "        'search': station_name,\n",
    "        'language': 'zh'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Ensure we catch HTTP errors\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sta_city_zh = {}\n",
    "with open('data/cnstation.csv', 'r', encoding='utf-8') as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    for ind, row in enumerate(csv_reader):\n",
    "        if ind == 0:\n",
    "            continue\n",
    "        sta_name = row[0].split(\"站\")[0]\n",
    "        sta_city_zh[sta_name] = (row[5], row[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [00:00<00:00, 130004.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "义乌\n",
      "三穗\n",
      "芷江\n",
      "婺源\n",
      "德兴\n",
      "五府山\n",
      "汉口\n",
      "汉川\n",
      "仙游\n",
      "惠安\n",
      "白沟\n",
      "秦皇岛\n",
      "双城北\n",
      "凤城东\n",
      "宾阳\n",
      "即墨北\n",
      "牟平\n",
      "荣成\n",
      "介休东\n",
      "高安\n",
      "宜春\n",
      "双峰北\n",
      "怀集\n",
      "恭城\n",
      "广宁\n",
      "云浮东\n",
      "郁南\n",
      "南江口\n",
      "白洋淀\n",
      "胜芳\n",
      "福田\n",
      "珲春\n",
      "璧山\n",
      "鲘门\n",
      "弋江\n",
      "繁昌西\n",
      "当涂东\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sta_translate = {}\n",
    "sta_city = {}\n",
    "sta_region = {}\n",
    "zhixiashi = [\"上海\", \"天津\", \"北京\", \"重庆\"]\n",
    "for sta in tqdm(data.keys()):\n",
    "    sta_translate[sta] = pinyin.get(sta, format=\"strip\")\n",
    "    tempsta = sta\n",
    "    while len(tempsta) >= 1 and tempsta not in sta_city_zh:\n",
    "        tempsta = tempsta[:-1]\n",
    "    if len(tempsta) == 0:\n",
    "        print(sta)\n",
    "        sta_city[sta] = pinyin.get(cities[sta], format=\"strip\")\n",
    "        sta_region[sta] = \"TODO\"\n",
    "        continue\n",
    "    if sta_city_zh[tempsta][0] in zhixiashi:\n",
    "        sta_city[sta] = pinyin.get(sta_city_zh[tempsta][0], format=\"strip\")\n",
    "        sta_region[sta] = pinyin.get(sta_city_zh[tempsta][0], format=\"strip\")\n",
    "    else:\n",
    "        sta_city[sta] = pinyin.get(sta_city_zh[tempsta][1], format=\"strip\")\n",
    "        sta_region[sta] = pinyin.get(sta_city_zh[tempsta][0], format=\"strip\")\n",
    "        \n",
    "    # resJson = fetch_wikidata(sta)\n",
    "    # if resJson is None or 'search' not in resJson:\n",
    "    #     sta_translate[sta] = pinyin.get(sta)\n",
    "    #     sta_city[sta] = pinyin.get(sta)\n",
    "    #     print(sta)\n",
    "    #     print(str(resJson))\n",
    "    # else:\n",
    "    #     try:\n",
    "    #         currInd = 0\n",
    "    #         while currInd <= len(resJson[\"search\"]) - 1 and resJson[\"search\"][currInd][\"display\"][\"label\"][\"language\"] != \"en\":\n",
    "    #             currInd += 1\n",
    "    #         if currInd >= len(resJson[\"search\"]):\n",
    "    #             sta_translate[sta] = pinyin.get(sta)\n",
    "    #             sta_city[sta] = pinyin.get(sta)\n",
    "    #             print(sta)\n",
    "    #             print(str(resJson))\n",
    "    #             continue\n",
    "    #         label = resJson[\"search\"][currInd][\"label\"]\n",
    "    #         if \"description\" in resJson[\"search\"][currInd]:\n",
    "    #             description = resJson[\"search\"][currInd][\"description\"]\n",
    "    #         else:\n",
    "    #             description = None\n",
    "    #     except Exception as e:\n",
    "    #         print(\"Error: \" + str(e) + \", Json: \" + str(resJson))\n",
    "\n",
    "    #     try:\n",
    "    #         if \"Railway Station\" in label:\n",
    "    #             staEng = label.split(\"Railway Station\")[0].strip()\n",
    "    #             if description is not None:\n",
    "    #                 tempDes = description.split(\",\")[0].strip()\n",
    "    #                 print(sta, tempDes)\n",
    "    #                 staCity = tempDes.split()[-1]\n",
    "    #             else:\n",
    "    #                 staCity = staEng\n",
    "    #         elif \"railway station\" in label:\n",
    "    #             staEng = label.split(\"railway station\")[0].strip()\n",
    "    #             if description is not None:\n",
    "    #                 tempDes = description.split(\",\")[0].strip()\n",
    "    #                 print(sta, tempDes)\n",
    "    #                 tempDesSplit = tempDes.split()\n",
    "    #                 if tempDesSplit[-1] == \"District\" or tempDesSplit[-1] == \"County\" or tempDesSplit[-1] == \"Zoo\" or tempDesSplit[-1] == \"Subdistrict\":\n",
    "                        \n",
    "    #                 staCity = tempDes.split()[-1]\n",
    "    #             else:\n",
    "    #                 staCity = staEng\n",
    "    #         else:\n",
    "    #             staEng = label\n",
    "    #             staCity = label\n",
    "    #         sta_translate[sta] = staEng\n",
    "    #         sta_city[sta] = staCity\n",
    "    #     except Exception as e:\n",
    "    #         print(\"Station: \" + sta + \", Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sta_translate_v2.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(sta_translate, f, ensure_ascii=False)\n",
    "# with open('sta_city_v2.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(sta_city, f, ensure_ascii=False)\n",
    "# with open('sta_region_v2.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(sta_region, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sta_cities.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(cities, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sta_cities.json', 'r', encoding='utf-8') as f:\n",
    "    cities = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sta_city_v2.json', 'r', encoding='utf-8') as f:\n",
    "    sta_city = json.load(f)\n",
    "with open('sta_region_v2.json', 'r', encoding='utf-8') as f:\n",
    "    sta_region = json.load(f)\n",
    "with open('sta_translate_v2.json', 'r', encoding='utf-8') as f:\n",
    "    sta_translate = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_sta = {}\n",
    "for sta, city in sta_city.items():\n",
    "    if city not in cities_to_sta:\n",
    "        cities_to_sta[city] = []\n",
    "    cities_to_sta[city].append(sta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_to_sta = {}\n",
    "for sta, region in sta_region.items():\n",
    "    if region not in regions_to_sta:\n",
    "        regions_to_sta[region] = []\n",
    "    regions_to_sta[region].append(sta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_sta = {k: v for k, v in sorted(cities_to_sta.items(), key = lambda x: x[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_to_sta = {k: v for k, v in sorted(regions_to_sta.items(), key = lambda x: x[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cities_sta_v2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(cities_to_sta, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('regions_sta_v2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(regions_to_sta, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_cities = list(cities_to_sta.keys())\n",
    "all_regions = list(regions_to_sta.keys())\n",
    "with open('processed_routes.csv', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"from_city,from_region_id,from_region_name,to_city,to_region_id,to_region_name,type,from_station,to_station,duration,distance,cost,route_id,route_name,enabled\\n\")\n",
    "    for sta1, vtemp in data.items():\n",
    "        for sta2, vals in vtemp.items():\n",
    "            f.write(sta_city[sta1] + \",\")\n",
    "            f.write(str(all_regions.index(sta_region[sta1]) + 1000) + \",\")\n",
    "            f.write(sta_region[sta1] + \",\")\n",
    "            f.write(sta_city[sta2] + \",\")\n",
    "            f.write(str(all_regions.index(sta_region[sta2]) + 1000) + \",\")\n",
    "            f.write(sta_region[sta2] + \",\")\n",
    "            f.write(\"train,\")\n",
    "            f.write(sta_translate[sta1] + \",\")\n",
    "            f.write(sta_translate[sta2] + \",\")\n",
    "            f.write(str(vals[0]) + \":\" + (str(vals[1]) if vals[1] >= 10 else (\"0\" + str(vals[1]))) + \",\")\n",
    "            f.write(str(vals[3]) + \",\")\n",
    "            f.write(str(vals[2]) + \",\")\n",
    "            f.write(vals[4] + \",\" + vals[4] + \",\" + \"True\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
